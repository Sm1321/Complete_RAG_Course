{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8e7d68c",
   "metadata": {},
   "source": [
    "### ðŸ§  What is Query Decomposition?\n",
    "Query decomposition is the process of taking a complex, multi-part question and breaking it into simpler, atomic sub-questions that can each be retrieved and answered individually.\n",
    "\n",
    "#### âœ… Why Use Query Decomposition?\n",
    "\n",
    "- Complex queries often involve multiple concepts\n",
    "\n",
    "- LLMs or retrievers may miss parts of the original question\n",
    "\n",
    "- It enables multi-hop reasoning (answering in steps)\n",
    "\n",
    "- Allows parallelism (especially in multi-agent frameworks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b23a442b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\MY_Folder\\Complete_RAG_Course\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.runnables import RunnableSequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76de0145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load and embed the document\n",
    "loader = TextLoader(\"langchain_crewai_dataset.txt\")\n",
    "docs = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.from_documents(chunks, embedding)\n",
    "retriever = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 4, \"lambda_mult\": 0.7})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43149d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x00000224166B6E90>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000224166B7890>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "llm = init_chat_model(model=\"groq:llama-3.1-8b-instant\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0af1982d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Query decomposition\n",
    "decomposition_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are an AI assistant. Decompose the following complex question into 2 to 4 smaller sub-questions for better document retrieval.\n",
    "\n",
    "Question: \"{question}\"\n",
    "\n",
    "Sub-questions:\n",
    "\"\"\")\n",
    "decomposition_chain = decomposition_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9797ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How does LangChain use memory and agents compared to CrewAI?\"\n",
    "decomposition_question = decomposition_chain.invoke({\"question\": query})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b4819b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To better understand the context and retrieve relevant information, I'll break down the complex question into 4 smaller sub-questions:\n",
      "\n",
      "1. **What is LangChain and what are its core features?**\n",
      "   This sub-question aims to understand the fundamental components and functionality of LangChain, allowing for a comparison with CrewAI.\n",
      "\n",
      "2. **How does LangChain utilize memory in its architecture?**\n",
      "   This sub-question delves into the specifics of LangChain's memory management, enabling a comparison with CrewAI's approach.\n",
      "\n",
      "3. **What are the key components and features of CrewAI?**\n",
      "   This sub-question focuses on understanding the core features and components of CrewAI, which will facilitate a meaningful comparison with LangChain.\n",
      "\n",
      "4. **How do LangChain and CrewAI differ in their use of agents or conversational interfaces?**\n",
      "   This sub-question explores the differences in agent-based architecture and conversational interfaces between LangChain and CrewAI, providing valuable insights into their distinct approaches.\n"
     ]
    }
   ],
   "source": [
    "print(decomposition_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5be04719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: QA chain per sub-question\n",
    "qa_prompt = PromptTemplate.from_template(\"\"\"\n",
    "Use the context below to answer the question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {input}\n",
    "\"\"\")\n",
    "qa_chain = create_stuff_documents_chain(llm=llm, prompt=qa_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26c735b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Full RAG pipeline logic\n",
    "def full_query_decomposition_rag_pipeline(user_query):\n",
    "    # Decompose the query\n",
    "    sub_qs_text = decomposition_chain.invoke({\"question\": user_query})\n",
    "    sub_questions = [q.strip(\"-â€¢1234567890. \").strip() for q in sub_qs_text.split(\"\\n\") if q.strip()]\n",
    "    \n",
    "    results = []\n",
    "    for subq in sub_questions:\n",
    "        docs = retriever.invoke(subq)\n",
    "        result = qa_chain.invoke({\"input\": subq, \"context\": docs})\n",
    "        results.append(f\"Q: {subq}\\nA: {result}\")\n",
    "    \n",
    "    return \"\\n\\n\".join(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac50f32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Final Answer:\n",
      "\n",
      "Q: To better understand the comparison between LangChain and CrewAI, I would decompose the complex question into the following 4 smaller sub-questions:\n",
      "A: It seems you've started your sub-questions. Here are the four smaller sub-questions to help compare LangChain and CrewAI, based on the context provided:\n",
      "\n",
      "1. How do LangChain agents operate?\n",
      "2. What tasks does LangChain handle in a hybrid system?\n",
      "3. What tasks does CrewAI manage in a hybrid system?\n",
      "4. How do CrewAI and LangChain collaborate in a hybrid system?\n",
      "\n",
      "Q: **What is the memory architecture used in LangChain?**\n",
      "A: The memory architecture used in LangChain includes modules like ConversationBufferMemory and ConversationSummaryMemory. These modules allow the LLM to maintain awareness of previous conversation turns or summarize long interactions to fit within token limits.\n",
      "\n",
      "Q: This sub-question aims to understand the structure and functionality of LangChain's memory, which will help in comparing it with CrewAI's memory architecture\n",
      "A: Based on the provided context, here's what we can understand about the structure and functionality of LangChain's memory:\n",
      "\n",
      "**Memory Modules:** LangChain offers two types of memory modules: \n",
      "1. **ConversationBufferMemory**: This module allows the LLM to maintain awareness of previous conversation turns. \n",
      "2. **ConversationSummaryMemory**: This module summarizes long interactions to fit within token limits.\n",
      "\n",
      "**Functionality:**\n",
      "- Maintains awareness of previous conversation turns (ConversationBufferMemory)\n",
      "- Summarizes long interactions to fit within token limits (ConversationSummaryMemory)\n",
      "\n",
      "These memory modules seem to be designed to help the LLM (Large Language Model) keep track of conversation history and manage its token limits, which is essential for efficient and effective conversation flow.\n",
      "\n",
      "Q: **How do agents interact with memory in LangChain?**\n",
      "A: According to the provided context, LangChain agents interact with memory in a context-aware manner across steps. This implies that agents can access, update, and utilize information stored in memory while executing a sequence of tool invocations to achieve a goal.\n",
      "\n",
      "Q: This sub-question will provide insight into the agent's role in LangChain and how they utilize memory to process information\n",
      "A: Based on the provided context, the LangChain agent uses memory in a context-aware manner to process information across steps. This means that the agent can retrieve and utilize previously obtained information to inform its decisions and actions in subsequent steps.\n",
      "\n",
      "Q: **What is the memory architecture used in CrewAI?**\n",
      "A: The provided context does not mention the memory architecture used in CrewAI. However, it does provide information on what CrewAI is, its features, and its applications.\n",
      "\n",
      "Q: This sub-question is crucial to establish a basis for comparison with LangChain. By understanding CrewAI's memory architecture, we can identify key differences and similarities\n",
      "A: Unfortunately, the context you provided does not give clear information about CrewAI's memory architecture. \n",
      "\n",
      "However, I can infer some possible differences between LangChain and CrewAI's memory architecture based on the information given.\n",
      "\n",
      "It is mentioned that LangChain offers memory modules like ConversationBufferMemory and ConversationSummaryMemory. However, there is no direct mention of CrewAI's memory architecture or its memory modules.\n",
      "\n",
      "One possible difference could be that CrewAI may not have its own built-in memory modules like LangChain, but instead relies on LangChain's memory modules or has a different approach to managing memory. However, this is purely speculative and cannot be confirmed based on the provided context.\n",
      "\n",
      "To establish a basis for comparison with LangChain, it would be necessary to consult additional information about CrewAI's memory architecture, or to look for updates from the developers or documentation.\n",
      "\n",
      "Q: **How do agents interact with memory in CrewAI, and what are the implications for workflow efficiency?**\n",
      "A: Unfortunately, I don't see any information provided in the context about how agents interact with memory in CrewAI or the implications for workflow efficiency. The context mainly focuses on the collaborative capabilities and structured workflows of CrewAI agents, but does not provide specific details about memory interaction.\n",
      "\n",
      "However, I can provide a general answer based on the general concept of multi-agent systems and possible implications for workflow efficiency:\n",
      "\n",
      "In CrewAI, agents may interact with memory in various ways, such as:\n",
      "\n",
      "1. **Context sharing**: Agents may share context and knowledge with each other, which can improve workflow efficiency by reducing redundant tasks and enabling more informed decision-making.\n",
      "2. **Knowledge base**: Each agent may have its own knowledge base, which can be used to store and retrieve information relevant to its role and tasks. This can improve workflow efficiency by allowing agents to access and use relevant information quickly.\n",
      "3. **Task management**: Agents may use memory to manage tasks, such as keeping track of completed tasks, pending tasks, and deadlines. This can improve workflow efficiency by ensuring that tasks are completed on time and that agents are aware of their responsibilities.\n",
      "\n",
      "The implications for workflow efficiency can be significant, including:\n",
      "\n",
      "1. **Improved task allocation**: Agents can work together more efficiently by allocating tasks based on their skills and expertise, reducing the risk of task duplication and errors.\n",
      "2. **Enhanced decision-making**: Agents can make more informed decisions by sharing context and knowledge, leading to better outcomes and reduced decision-making time.\n",
      "3. **Increased productivity**: Agents can work more efficiently by leveraging each other's strengths and avoiding redundant tasks, leading to increased productivity and better workflow efficiency.\n",
      "\n",
      "Please note that these are general implications and may not be specific to CrewAI. A more detailed analysis of CrewAI's memory interaction and workflow efficiency would require more information about the system.\n",
      "\n",
      "Q: This sub-question compares the agent-memory interaction in both LangChain and CrewAI, focusing on workflow efficiency. This will help in understanding how each framework handles memory and agent interactions to achieve their respective goals\n",
      "A: Based on the provided context, here's the answer to the question:\n",
      "\n",
      "LangChain and CrewAI have different roles in handling agent-memory interactions. LangChain is primarily responsible for planning out a sequence of tool invocations to achieve a goal, which includes dynamic decision-making, branching logic, and context-aware memory use across steps. \n",
      "\n",
      "On the other hand, CrewAI, which is compatible with LangChain, focuses on managing role-based collaboration. This suggests that CrewAI does not directly handle agent-memory interactions but rather coordinates the interactions of different agents or roles within a collaborative framework.\n",
      "\n",
      "Therefore, when comparing the agent-memory interaction in both LangChain and CrewAI, focusing on workflow efficiency, it appears that LangChain is responsible for handling memory and agent interactions to achieve its goals, while CrewAI facilitates collaboration among agents and roles.\n",
      "\n",
      "Q: By answering these sub-questions, we can gain a deeper understanding of the differences and similarities between LangChain and CrewAI, providing valuable insights for document retrieval and comparison\n",
      "A: Based on the provided context, here are some sub-questions that can help us gain a deeper understanding of the differences and similarities between LangChain and CrewAI:\n",
      "\n",
      "1. **What specific features of LangChain enable it to handle retrieval and tool wrapping, and how does it compare to CrewAI's capabilities in these areas?**\n",
      "\n",
      "This question can help us understand how LangChain's features, such as hybrid retrieval and vector database integration, compare to CrewAI's role-based collaboration management.\n",
      "\n",
      "2. **How do LangChain and CrewAI interact with each other in hybrid systems, and what are the benefits of this integration?**\n",
      "\n",
      "This question can help us understand the strengths and weaknesses of combining LangChain's retrieval capabilities with CrewAI's collaboration management features.\n",
      "\n",
      "3. **What are the differences in approach between LangChain's keyword-based and embedding-based retrieval methods, and how do they impact recall and search results?**\n",
      "\n",
      "This question can help us understand the trade-offs between LangChain's hybrid retrieval approach and other possible retrieval methods.\n",
      "\n",
      "4. **How do vector databases like FAISS, Chroma, Pinecone, and Weaviate enhance LangChain's semantic search capabilities, and what are the implications for Retrieval-Augmented Generation (RAG)?**\n",
      "\n",
      "This question can help us understand the benefits of using vector databases with LangChain and how they impact RAG applications.\n",
      "\n",
      "5. **In what scenarios would a hybrid system combining LangChain and CrewAI be more beneficial than using either tool separately, and what are the potential limitations of such a system?**\n",
      "\n",
      "This question can help us understand the potential use cases and limitations of combining LangChain and CrewAI for document retrieval and comparison tasks.\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Run\n",
    "query = \"How does LangChain use memory and agents compared to CrewAI?\"\n",
    "final_answer = full_query_decomposition_rag_pipeline(query)\n",
    "print(\"âœ… Final Answer:\\n\")\n",
    "print(final_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "complete-rag-course (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
